2. hét:
A megbeszéltek alapján létrehoztam a gites repomat, amiben egyelőre a readme.md nagyjából üres, a dokumentációknak szánt mappába pedig elhelyeztem a haladási naplót, amit jelenleg írok. A héten megnéztem a csatolt videót és a mellékelt leírást is a CameraX APIról, illetve az MLKitről. 
3. hét:
Elkezdtem egy próba projektet, amiben először is kipróbálom a CameraX-et, ezen leírás alapján: https://developer.android.com/codelabs/camerax-getting-started#0
Még nem tudtam végigcsinálni, mert a gépem nem bírja el az emulátort, és csak vasárnap tudok szerezni egy régebbi androidos telefont a teszteléshez. 
A madár felismerésről találtam egy konkrét videót az Android Developers csatornáján, illetve több nem ennyire specifikus ML kit és TensorFlow témájút, amik sokat segítettek a megértésben, illetve valószínüleg akkor fognak többet, amikor már ténylegesen tudok foglalkozni az app megvalósításával.
4. hét: 
Elkészültem a madár felismerős és fotózós projekttel. Az app felismeri a madarakat, és le is fotózza őket, CameraX, illetve ML Kit Object detection segítségével. 
5. hét: 
A Google által létrehozott MLKIt mintaprojekteket néztem át. Ezek nem szerepelnek a gites repoban.
6. hét:
A konzultáción megbeszélt "tánc tanuló" apphoz kerestem megfelelő tánclépéseket, végül a "the running man" névre keesztelt shuffle lépést választottam. Ehhez vázoltam a szükséges állapotokat, és megnéztem, hogy mely csomópontok összehasonlítását lehet használni az állapotok felismeréséhez. Egyelőre az applikációhoz csak a CameraX-et valósítottam meg, tehát a kamera már működik, illetve a pose detection is hozzá lett adva, de még lényegi előrelépés csak elméleti síkon történt. Az applikációt a későbbiekben két fázisra tervezem bontani. Egy tanító és egy próba fázisra, a tanító részhez gyűjtöttem képeket, amik alapján a felhasználó le tudja utánozni az egyes állapotokat, melyek alapán az app visszajelezhet, hogy azokat sikerült e megfelelően alkalmaznia a felhasználónak. A próba fázisnál az app a teljes koreográfia eltáncolását nézi majd, és az alapján jelez vissza, tehát végigmegy a teljes állapotgépen.
7. hét:
Elkezdtem felépíteni az app szerkezetét, és logikáját, elméletben már a posedetection init része működik, illetve az első lépést is felismeri. Ezt tesztelni még nem sikerült. 
A kamera fölött megjelenő képek változtatásával kissé elakadtam. Még nem találtam meg a megfelelő megoldást, hogy dinamikusan tudjam frissíteni minden lépés után a megjelenő képet, illetve, hogy valamilyen feedback képet is megjelenítsek a felhasználó számára a sikeres művelet után. (Tehát, ha egy lépés sikeres volt, megjelenjen a képernyőn egy pipa vagy like jel.) 
8. hét:
A lépéseket ellenőrző függvények elkészültek, helyesen működnek, a képek is változnak lépésenként, bár az állapotgép nem működik tökéletesen. A jővőhéten egy újabb megközelítésben fogok nekifutni. 